import os
import cv2
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from decord import VideoReader, cpu
import torchvision.models.video as models
from settings import RTSP_URL, CAPTURE_INTERVAL  # settings.py íŒŒì¼ì´ ìˆë‹¤ê³  ê°€ì •


# -----------------------------------------------------------
# 1. ì´ì „ ì½”ë“œì—ì„œ ì‚¬ìš©í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ë° ëª¨ë¸ ì •ì˜
# -----------------------------------------------------------

def load_video_frames(video_path, num_frames=16):
    """
    ì˜ìƒ íŒŒì¼ì—ì„œ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        vr = VideoReader(video_path, ctx=cpu(0))
        total_frames = len(vr)
        # ì´ í”„ë ˆì„ ìˆ˜ì— ë§ì¶° ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ 16ê°œ í”„ë ˆì„ ì„ íƒ
        indices = torch.linspace(0, total_frames - 1, num_frames).long()
        frames_nd = vr.get_batch(indices)
        frames = torch.from_numpy(frames_nd.asnumpy())
        frames = frames.permute(0, 3, 1, 2)  # [T, C, H, W] -> [16, 3, H, W]
        return frames / 255.0
    except Exception as e:
        print(f"ì˜ìƒ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


# 3D CNN íŠ¹ì§• ì¶”ì¶œê¸°
model_3d = models.r2plus1d_18(weights=models.R2Plus1D_18_Weights.KINETICS400_V1)
model_3d.eval()
feature_extractor = torch.nn.Sequential(*list(model_3d.children())[:-1])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
feature_extractor.to(device)


def extract_feature(frames):
    """
    3D CNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ìƒ í”„ë ˆì„ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if frames is None:
        return None

    # decordë¡œ ë¶ˆëŸ¬ì˜¨ í”„ë ˆì„ í¬ê¸°ë¥¼ 3D CNNì— ë§ê²Œ ì¡°ì ˆ
    frames = torch.nn.functional.interpolate(frames, size=(112, 112))
    frames = frames.unsqueeze(0).permute(0, 2, 1, 3, 4)
    with torch.no_grad():
        feat = feature_extractor(frames.to(device))
    return feat.view(-1).cpu().numpy().reshape(1, -1)  # [1, 512] í˜•íƒœë¡œ ë°˜í™˜


# Residual MLP ë¶„ë¥˜ê¸° ëª¨ë¸
class ResidualMLP(nn.Module):
    def __init__(self, dropout_rate=0.5):
        super().__init__()
        self.fc1 = nn.Linear(512, 64)  # <-- ì…ë ¥ ì°¨ì›ì„ 512ë¡œ ë³€ê²½
        self.norm1 = nn.LayerNorm(64)
        self.dropout1 = nn.Dropout(dropout_rate)

        self.fc2 = nn.Linear(64, 64)
        self.norm2 = nn.LayerNorm(64)
        self.dropout2 = nn.Dropout(dropout_rate)

        self.fc3 = nn.Linear(64, 32)
        self.norm3 = nn.LayerNorm(32)
        self.dropout3 = nn.Dropout(dropout_rate)

        self.out = nn.Linear(32, 2)

    def forward(self, x):
        x = F.relu(self.norm1(self.fc1(x)))
        x = self.dropout1(x)
        residual = x
        x = F.relu(self.norm2(self.fc2(x)))
        x = self.dropout2(x)
        x = x + residual
        x = F.relu(self.norm3(self.fc3(x)))
        x = self.dropout3(x)
        return self.out(x)


# -----------------------------------------------------------
# 2. ëª¨ë¸ ë¡œë“œ (MLPë§Œ)
# -----------------------------------------------------------

# MLP ëª¨ë¸ ë¡œë“œ
model_clf = ResidualMLP().to(device)
try:
    model_clf.load_state_dict(torch.load('best_residual_mlp_model.pth', map_location=device))
    model_clf.eval()
    print("âœ… MLP ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
except FileNotFoundError:
    print("âŒ best_residual_mlp_model.pth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµì‹œí‚¤ê³  ì €ì¥í•˜ì„¸ìš”.")
    exit()


# -----------------------------------------------------------
# 3. RTSP ì˜ìƒ ìº¡ì²˜ ë° ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸
# -----------------------------------------------------------

def analyze_captured_video(video_path):
    """
    ì €ì¥ëœ ì˜ìƒì„ ë¶ˆëŸ¬ì™€ í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ” ì˜ìƒ ë¶„ì„ ì‹œì‘: {video_path}")

    # 1. decordë¡œ ì˜ìƒ í”„ë ˆì„ ë¡œë“œ
    frames = load_video_frames(video_path)
    if frames is None:
        return

    # 2. 3D CNNì„ ì‚¬ìš©í•´ íŠ¹ì§• ì¶”ì¶œ (512ì°¨ì›)
    feature_vector = extract_feature(frames)

    # 3. MLP ë¶„ë¥˜ê¸°ë¡œ ì˜ˆì¸¡ (PCA ë‹¨ê³„ ì‚­ì œ)
    with torch.no_grad():
        x = torch.from_numpy(feature_vector).float().to(device)
        output = model_clf(x)
        probabilities = F.softmax(output, dim=1)
        pred_label_idx = torch.argmax(probabilities, dim=1).item()

    # 4. ê²°ê³¼ ì¶œë ¥
    label_map_inv = {0: "ì •ìƒ (Normal)", 1: "ë¹„ì •ìƒ (Abnormal)"}
    predicted_label = label_map_inv[pred_label_idx]

    print(f"\nâœ¨ ì˜ˆì¸¡ ê²°ê³¼: {predicted_label}")
    print(f"í™•ë¥ : {probabilities.cpu().numpy()[0]}")


def preview_captured_video(video_path):
    """
    ì €ì¥ëœ ì˜ìƒì„ í™”ë©´ì— ë„ì›Œ ì„ì‹œë¡œ ì¬ìƒí•©ë‹ˆë‹¤.
    'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.
    """
    print(f"ğŸ“º '{video_path}' ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°. 'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”.")
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ ì˜ìƒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        cv2.imshow('Captured Video Preview', frame)
        if cv2.waitKey(25) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()


# RTSP ìº¡ì²˜ ë° ë¶„ì„ ë©”ì¸ ë¡œì§
rtsp_url = RTSP_URL
capture_duration = 4  # 4ì´ˆ
output_file = 'output_4sec.mp4'

cap = cv2.VideoCapture(rtsp_url)
if not cap.isOpened():
    print("âŒ RTSP ì—°ê²° ì‹¤íŒ¨. URLì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
else:
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    fps = 30
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    out = cv2.VideoWriter(output_file, fourcc, fps, (width, height))

    start_time = time.time()
    print(f"â–¶ï¸ {capture_duration}ì´ˆ ë™ì•ˆ ì˜ìƒ ìº¡ì²˜ ì‹œì‘...")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("í”„ë ˆì„ ìˆ˜ì‹  ì‹¤íŒ¨")
            break

        out.write(frame)
        if time.time() - start_time > capture_duration:
            break

    cap.release()
    out.release()
    print(f"âœ… {capture_duration}ì´ˆì§œë¦¬ ì˜ìƒ ì €ì¥ ì™„ë£Œ: {output_file}")

    # ìº¡ì²˜ëœ ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°
    preview_captured_video(output_file)

    # ì´ê±° ì˜ìƒ ì €ì¥í•˜ëŠ”ê±°ëŠ” ë¹„ì •ìƒë§Œ í•˜ê³ 
    # ë‚˜ì¤‘ì— ì½”ë“œ êµ¬ê¸€ í´ë¼ìš°ë“œì— ì—…ë¡œë“œí•˜ë©´, fireDBì— ì˜ìƒì €ì¥ í• ê±´ë° ê·¸ë ‡ê²Œí•  ì½”ë“œ ì¶”ê°€
    # ì˜ìƒ ì €ì¥ í›„ ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ
    analyze_captured_video(output_file)
