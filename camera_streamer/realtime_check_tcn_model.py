import os
import cv2
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models.video as models
import numpy as np
import threading
import queue
from decord import VideoReader, cpu
from settings import RTSP_URL, CAPTURE_INTERVAL  # settings.py íŒŒì¼ì´ ìˆë‹¤ê³  ê°€ì •

# -----------------------------------------------------------
# 1. ëª¨ë¸ ì •ì˜ ë° ë¡œë“œ
# -----------------------------------------------------------

# 3D CNN íŠ¹ì§• ì¶”ì¶œê¸°
model_3d = models.r2plus1d_18(weights=models.R2Plus1D_18_Weights.KINETICS400_V1)
model_3d.eval()
feature_extractor = torch.nn.Sequential(*list(model_3d.children())[:-1])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
feature_extractor.to(device)


def extract_feature(frames_tensor):
    """
    3D CNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ì„œ í˜•íƒœì˜ í”„ë ˆì„ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # decordë¡œ ë¶ˆëŸ¬ì˜¨ í”„ë ˆì„ í¬ê¸°ë¥¼ 3D CNNì— ë§ê²Œ ì¡°ì ˆ
    frames = torch.nn.functional.interpolate(frames_tensor, size=(112, 112))
    frames = frames.unsqueeze(0).permute(0, 2, 1, 3, 4)
    with torch.no_grad():
        feat = feature_extractor(frames.to(device))
    return feat.view(-1).cpu().numpy().reshape(1, -1)


# TCN(Temporal Convolutional Network) ë¶„ë¥˜ê¸° ëª¨ë¸
class TCNBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, dilation_rate):
        super(TCNBlock, self).__init__()
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=(kernel_size - 1) * dilation_rate // 2,
                               dilation=dilation_rate)
        self.norm1 = nn.BatchNorm1d(out_channels)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(0.2)

        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=(kernel_size - 1) * dilation_rate // 2,
                               dilation=dilation_rate)
        self.norm2 = nn.BatchNorm1d(out_channels)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(0.2)

        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None

    def forward(self, x):
        residual = x
        if self.downsample:
            residual = self.downsample(residual)

        out = self.conv1(x)
        out = self.norm1(out)
        out = self.relu1(out)
        out = self.dropout1(out)

        out = self.conv2(out)
        out = self.norm2(out)
        out = self.relu2(out)
        out = self.dropout2(out)

        return F.relu(out + residual)


class TCNClassifier(nn.Module):
    def __init__(self, input_size=512, num_channels=[64, 128], kernel_size=3):
        super(TCNClassifier, self).__init__()
        self.blocks = nn.ModuleList([
            TCNBlock(input_size if i == 0 else num_channels[i - 1],
                     num_channels[i],
                     kernel_size,
                     2 ** i) for i in range(len(num_channels))
        ])
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(num_channels[-1], 2)

    def forward(self, x):
        # [batch_size, 512] -> [batch_size, 1, 512]
        x = x.unsqueeze(1)

        for block in self.blocks:
            x = block(x)

        x = self.pool(x).squeeze(-1)
        x = self.fc(x)

        return x


# TCN ëª¨ë¸ ë¡œë“œ
model_clf = TCNClassifier().to(device)
try:
    model_clf.load_state_dict(torch.load('best_tcn_model.pth', map_location=device))
    model_clf.eval()
    print("âœ… TCN ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
except FileNotFoundError:
    print("âŒ best_tcn_model.pth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµì‹œí‚¤ê³  ì €ì¥í•˜ì„¸ìš”.")
    exit()


# -----------------------------------------------------------
# 2. ë©€í‹°ìŠ¤ë ˆë”©ì„ ì‚¬ìš©í•œ RTSP ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
# -----------------------------------------------------------

class RTSPFrameGrabber(threading.Thread):
    def __init__(self, rtsp_url, frame_queue):
        threading.Thread.__init__(self)
        self.rtsp_url = rtsp_url
        self.frame_queue = frame_queue
        self.cap = cv2.VideoCapture(rtsp_url)
        self.running = True

    def run(self):
        print("ğŸ¥ RTSP í”„ë ˆì„ ìº¡ì²˜ ìŠ¤ë ˆë“œ ì‹œì‘...")
        while self.running:
            ret, frame = self.cap.read()
            if not ret:
                print("í”„ë ˆì„ ìˆ˜ì‹  ì‹¤íŒ¨ ë˜ëŠ” ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ. ì¬ì—°ê²° ì‹œë„...")
                self.cap.release()
                self.cap = cv2.VideoCapture(self.rtsp_url)
                time.sleep(1)
                continue

            # íì— í”„ë ˆì„ ì €ì¥
            self.frame_queue.put(frame)

    def stop(self):
        self.running = False
        self.cap.release()
        print("ğŸ›‘ RTSP í”„ë ˆì„ ìº¡ì²˜ ìŠ¤ë ˆë“œ ì¢…ë£Œ.")


def process_stream_continuously(rtsp_url, interval_sec=4, num_frames_per_interval=16):
    """
    RTSP ìŠ¤íŠ¸ë¦¼ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì½ê³ , ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    frame_queue = queue.Queue(maxsize=120)  # ìµœëŒ€ 120 í”„ë ˆì„ (4ì´ˆ) ë²„í¼

    # ìº¡ì²˜ ìŠ¤ë ˆë“œ ì‹œì‘
    grabber = RTSPFrameGrabber(rtsp_url, frame_queue)
    grabber.start()

    last_analysis_time = time.time()
    label_map_inv = {0: "ì •ìƒ (Normal)", 1: "ë¹„ì •ìƒ (Abnormal)"}
    frame_buffer = []

    # ë¹„ì •ìƒ ì´ë²¤íŠ¸ ì´ë¯¸ì§€ì™€ ì˜ìƒì„ ì €ì¥í•  í´ë” ìƒì„±
    abnormal_events_folder = 'abnormal_events'
    abnormal_videos_folder = 'abnormal_videos'
    os.makedirs(abnormal_events_folder, exist_ok=True)
    os.makedirs(abnormal_videos_folder, exist_ok=True)
    print(f"ğŸ“‚ ë¹„ì •ìƒ ì´ë²¤íŠ¸ ì´ë¯¸ì§€ëŠ” '{abnormal_events_folder}', ì˜ìƒì€ '{abnormal_videos_folder}' í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.")

    print("â–¶ï¸ ì‹¤ì‹œê°„ ë¶„ì„ ë©”ì¸ ìŠ¤ë ˆë“œ ì‹œì‘...")

    try:
        while True:
            # íì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (ë…¼ë¸”ë¡œí‚¹)
            try:
                frame = frame_queue.get(block=False)
                frame_buffer.append(frame)
            except queue.Empty:
                pass  # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ë‹¤ìŒ ë£¨í”„ë¡œ ë„˜ì–´ê°

            current_time = time.time()
            if current_time - last_analysis_time >= interval_sec:
                print(f"\n--- {interval_sec}ì´ˆ ì˜ìƒ ë¶„ì„ ì‹œì‘ ---")

                if len(frame_buffer) > 0:
                    frames_to_analyze = []
                    step = max(1, len(frame_buffer) // num_frames_per_interval)
                    for i in range(num_frames_per_interval):
                        idx = min(i * step, len(frame_buffer) - 1)
                        frames_to_analyze.append(frame_buffer[idx])

                    frames_np = np.array(frames_to_analyze)
                    frames_tensor = torch.from_numpy(frames_np).permute(0, 3, 1, 2).to(torch.float32) / 255.0

                    feature_vector = extract_feature(frames_tensor)

                    with torch.no_grad():
                        x = torch.from_numpy(feature_vector).float().to(device)
                        output = model_clf(x)
                        probabilities = F.softmax(output, dim=1)
                        pred_label_idx = torch.argmax(probabilities, dim=1).item()

                    predicted_label = label_map_inv[pred_label_idx]

                    print(f"âœ¨ ì˜ˆì¸¡ ê²°ê³¼: {predicted_label} | í™•ë¥ : {probabilities.cpu().numpy()[0]}")

                    if pred_label_idx == 1:
                        print("ğŸš¨ 'Abnormal' ê°ì§€! ëŒ€í‘œ ì´ë¯¸ì§€ì™€ ì˜ìƒì„ ì €ì¥í•©ë‹ˆë‹¤.")

                        # íŒŒì¼ ê²½ë¡œë¥¼ í´ë”ì™€ íŒŒì¼ëª…ìœ¼ë¡œ ì¡°í•©
                        timestamp = int(time.time())
                        image_filename = f'abnormal_event_{timestamp}.jpg'
                        video_filename = f'abnormal_video_{timestamp}.avi'

                        image_filepath = os.path.join(abnormal_events_folder, image_filename)
                        video_filepath = os.path.join(abnormal_videos_folder, video_filename)

                        # ì´ë¯¸ì§€ ì €ì¥
                        representative_frame = frame_buffer[len(frame_buffer) // 2]
                        cv2.imwrite(image_filepath, representative_frame)
                        print(f"ğŸ’¾ '{image_filepath}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

                        # ë¹„ë””ì˜¤ ì €ì¥
                        fourcc = cv2.VideoWriter_fourcc(*'XVID')
                        out = cv2.VideoWriter(video_filepath, fourcc, 20.0,
                                              (frame_buffer[0].shape[1], frame_buffer[0].shape[0]))
                        for frame in frame_buffer:
                            out.write(frame)
                        out.release()
                        print(f"ğŸ¥ '{video_filepath}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                else:
                    print("âš ï¸ í”„ë ˆì„ ë²„í¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ê°„ê²©ê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.")

                frame_buffer = []
                last_analysis_time = current_time

            # CPU ì ìœ ìœ¨ì„ ë‚®ì¶”ê¸° ìœ„í•œ sleep
            time.sleep(0.01)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¼ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        grabber.stop()
        grabber.join()
        cv2.destroyAllWindows()
        print("âœ… ëª¨ë“  ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œí–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    process_stream_continuously(RTSP_URL)
